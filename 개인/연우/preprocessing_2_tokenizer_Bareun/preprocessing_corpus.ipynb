{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LojblkE72qsR"
   },
   "source": [
    "## [환경설정] 전처리(2) tagger 및 tokenizer\n",
    "- bareun 형태소 분석기 API 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ie3-6eceP908"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# 바른의 리눅스 설치본 다운로드\n",
    "\n",
    "!curl -LJks -H \"uname:$(uname -a)\" https://bareun.ai/api/get -o bareun-linux.deb\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zI5rMg5ORQug"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'uname'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n",
      "'dpkg'은(는) 내부 또는 외부 명령, 실행할 수 있는 프로그램, 또는\n",
      "배치 파일이 아닙니다.\n"
     ]
    }
   ],
   "source": [
    "# 바른 설치\n",
    "\n",
    "!uname -a\n",
    "!dpkg -i bareun-linux.deb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ujG4VmRKRSO_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  392M    0 2112k    0     0  1631k      0  0:04:06  0:00:01  0:04:05 1641k\n",
      "  3  392M    3 12.6M    0     0  5645k      0  0:01:11  0:00:02  0:01:09 5663k\n",
      "  5  392M    5 23.3M    0     0  7269k      0  0:00:55  0:00:03  0:00:52 7285k\n",
      "  8  392M    8 33.6M    0     0  8019k      0  0:00:50  0:00:04  0:00:46 8033k\n",
      " 11  392M   11 44.2M    0     0  8552k      0  0:00:46  0:00:05  0:00:41 9160k\n",
      " 13  392M   13 54.9M    0     0  8936k      0  0:00:44  0:00:06  0:00:38 10.5M\n",
      " 16  392M   16 63.2M    0     0  9147k      0  0:00:43  0:00:07  0:00:36 10.5M\n",
      "curl: (23) Failure writing output to destination, passed 16384 returned 2289\n",
      "tar: could not chdir to '/opt/bareun'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 바른 모델이 GPU를 사용하기 위해서 다운받는 tensorflow 라이브러리\n",
    "\n",
    "!curl -O https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-2.9.1.tar.gz\n",
    "!tar -C /opt/bareun -xzf libtensorflow-gpu-linux-x86_64-2.9.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MV7QhRZrRaki"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BAREUN_ROOT=\"/opt/bareun\"\n",
      "env: LD_LIBRARY_PATH=\"/opt/bareun/lib\"\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAREUN_ROOT=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/bareun\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLD_LIBRARY_PATH=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/bareun/lib\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBAREUN_ROOT=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/bareun\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m LD_LIBRARY_PATH=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/bareun/lib\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m nohup /opt/bareun/bin/bareun&\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py:641\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[0;32m    640\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 641\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "%env BAREUN_ROOT=\"/opt/bareun\"\n",
    "%env LD_LIBRARY_PATH=\"/opt/bareun/lib\"\n",
    "!BAREUN_ROOT=\"/opt/bareun\" LD_LIBRARY_PATH=\"/opt/bareun/lib\" nohup /opt/bareun/bin/bareun&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJhQsWVFRceN"
   },
   "outputs": [],
   "source": [
    "!ps -ef | grep bareun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKVRddfvReka"
   },
   "outputs": [],
   "source": [
    "# Yeonwoo's API KEY\n",
    "\n",
    "!BAREUN_ROOT=\"/opt/bareun\" LD_LIBRARY_PATH=\"/opt/bareun/lib\" /opt/bareun/bin/bareun -reg koba-MDIHR4Q-BQUUMOY-TOGE42A-NDESPKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAQSf-yvRg__"
   },
   "outputs": [],
   "source": [
    "!pip install -U bareunpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZrIH-LaRjdR"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import bareunpy as brn\n",
    "import google.protobuf.text_format as tf\n",
    "\n",
    "from bareunpy import Tagger   # 이하 추가: 품사 tagger\n",
    "from bareunpy import Tokenizer  # 형태소 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6o5dj3P2teO"
   },
   "source": [
    "## train 데이터 불러오기 및 클래스 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oByWRGtiz1qq"
   },
   "outputs": [],
   "source": [
    "# train data\n",
    "# train_data_path =\"~/aiffel/dktc/data/train.csv\"\n",
    "\n",
    "# train_data_path = \"/content/drive/MyDrive/aiffel/dktc/data/train.csv\"\n",
    "train_data_path = \"D:/DLThon/맞춤법교정/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krYcHioD0NPd"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SjERd8aMEgd"
   },
   "source": [
    "### [train] 예측 대상 클래스 인코딩 (총4가지)\n",
    "- '협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZlcgL-tFLk1_"
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['협박 대화', '갈취 대화', '직장 내 괴롭힘 대화', '기타 괴롭힘 대화']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASS_NAMES)\n",
    "\n",
    "train_data['class'] = encoder.transform(train_data['class'])\n",
    "\n",
    "corpus = train_data[\"conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzakKZKo0mhR"
   },
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAUkoSHF4ID9"
   },
   "source": [
    "## [일반 대화] 일반 대화 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7O6XmbgxiQbT"
   },
   "outputs": [],
   "source": [
    "# 일반 대화 데이터셋 4000개\n",
    "# conversations_casual_path = '/content/drive/MyDrive/aiffel/dktc/data/sampled_dataset.csv'  # text file\n",
    "conversations_casual_path = 'D:/DLThon/맞춤법교정/sampled_dataset.csv'\n",
    "\n",
    "# 인코딩을 명시적으로 지정하여 파일 읽기\n",
    "casual_data = pd.read_csv(conversations_casual_path, encoding='utf-8-sig')  # UTF-8 BOM(Byte Order Mark) 파일 읽어 옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQL49AlyiTy3"
   },
   "outputs": [],
   "source": [
    "# 데이터 확인\n",
    "print(casual_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_vOPzm8iVaY"
   },
   "outputs": [],
   "source": [
    "casual_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu3q67eajN9r"
   },
   "source": [
    "## [공통] - 정규표현식, 맞춤법 검사기 함수 정의\n",
    "- 정규표현식 :\n",
    "- 최종 맞춤법 검사기 API : py-hanspell (비교 위해 ET5 기반 모델을 함께 제시함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlyHMEcom6mn"
   },
   "source": [
    "### 정규표현식 적용 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "N1D__Z7jkLK5"
   },
   "outputs": [],
   "source": [
    "# 정규표현식 적용 함수\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EL--FcbhkM_A"
   },
   "outputs": [],
   "source": [
    "def load_conversations_from_df(corpus_df):\n",
    "    conv_data = []\n",
    "\n",
    "    for index, row in corpus_df.iterrows():  # 수정된 부분\n",
    "        conversations = row['conversation']  # 수정된 부분\n",
    "        conv_data.append(preprocess_sentence(conversations))  # 전처리 함수 적용\n",
    "    return conv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GLdLucGVjyhF"
   },
   "outputs": [],
   "source": [
    "# 정규표현식 적용 함수\n",
    "# data 구성이 다르므로 별도 함수 정의\n",
    "\n",
    "# 전처리 함수 수정 ver. casual data\n",
    "\n",
    "def preprocess_sentence_Casual(sentence):     # 추후 사용자 지정 단어 사전 구축하기 : 'ㅠㅠ', 'ㅎㅎ', 'ㅋㅋ', '^^', '넹넹', '웅웅' 등\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # 'ㅠㅠ', 'ㅎㅎ', 'ㅋㅋ'의 반복을 각각 'ㅠㅠ', 'ㅎㅎ', 'ㅋㅋ'로 통일\n",
    "    sentence = re.sub(r\"(ㅠ)+\", \"ㅠㅠ\", sentence)  # 'ㅠㅠ' 이상 반복을 'ㅠㅠ'로\n",
    "    sentence = re.sub(r\"(ㅎ)+\", \"ㅎㅎ\", sentence)  # 'ㅎㅎ' 이상 반복을 'ㅎㅎ'로\n",
    "    sentence = re.sub(r\"(ㅋ)+\", \"ㅋㅋ\", sentence)  # 'ㅋㅋ' 이상 반복을 'ㅋㅋ'로\n",
    "    # 한글, 알파벳, 문장 부호 및 'ㅠㅠ', 'ㅎㅎ', 'ㅋㅋ', '^^'는 남기고 나머지는 공백으로 대체\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,ㅠㅠㅎㅎㅋㅋ^^]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3lHW5jnmQsN"
   },
   "source": [
    "### 맞춤법 교정기 py-hanspell, ET5(개인) 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VOGXUiJmP4Z"
   },
   "outputs": [],
   "source": [
    "def textLine_from_row(text_from_row) :  # 각 row 데이터 -> 하나의 리스트, 발화 단위 분리 (각 원소 = 하나의 발화)\n",
    "    text_line = []\n",
    "    for i in range(0,text_from_row.size) :\n",
    "        text = text_from_row.iloc[i, 0].split('\\n')\n",
    "        text_line.append(text)\n",
    "    return text_line\n",
    "\n",
    "\n",
    "def py_hanspell_spell_correct(text_line) :  # 분리된 각 line의 여러 문장을 읽어 옴 (하나의 발화 단위 text)\n",
    "    checked_data_1 = []\n",
    "    print('py-hanspell 텍스트 교정 시작')\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(len(text_line)) :\n",
    "        for k in range(len(text_line[i])):  # 해당 인덱스 내 리스트/문자열을 순회\n",
    "            result = spell_checker.check(f'{text_line[i][k]}').checked  # 맞춤법 검사\n",
    "            checked_data_1.append(result)  # 결과 저장\n",
    "\n",
    "    end = time.time()\n",
    "    length = end - start\n",
    "    print(length, '초 걸렸습니다.')\n",
    "    return checked_data_1\n",
    "\n",
    "\n",
    "def BaseOnPLMmodelET5_spell_correct(text_line) :  # 분리된 각 line의 여러 문장을 읽어 옴 (하나의 발화 단위 text)\n",
    "    checked_data_2 = []\n",
    "    print('BaseOnPLMmodelET5 텍스트 교정 시작')\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(len(text_line)) :\n",
    "        for k in range(len(text_line[i])):  # 해당 인덱스 내 리스트/문자열을 순회\n",
    "            result = typos_corrector(f'{text_line[i][k]}') # 맞춤법 검사\n",
    "            checked_data_2.append(result)  # 결과 저장\n",
    "\n",
    "    end = time.time()\n",
    "    length = end - start\n",
    "    print(length, '초 걸렸습니다.')\n",
    "    return checked_data_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PiVdWqyjFFe"
   },
   "source": [
    "## [1-1] train 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKfb6pRAkQlE"
   },
   "outputs": [],
   "source": [
    "conv_data = load_conversations_from_df(train_data)\n",
    "\n",
    "print('전체 샘플 수:', len(conv_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LKf0i5lkRL0"
   },
   "outputs": [],
   "source": [
    "# 한국어 전처리 함수 (정규표현식) 적용 결과\n",
    "\n",
    "print('전처리 후의 1081번째 샘플: {}'.format(conv_data[1081]))\n",
    "print('전처리 후의 500번째 샘플: {}'.format(conv_data[500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVIVx1FGiI2V"
   },
   "source": [
    "## [1-2] 일반 대화 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhV2GWW_vbXk"
   },
   "outputs": [],
   "source": [
    "# 정규표현식 적용\n",
    "preprocessed_casual_data = casual_data.copy()\n",
    "\n",
    "for i in range(len(preprocessed_casual_data)):\n",
    "    preprocessed_casual_data.iloc[i, 0] = preprocess_sentence_Casual(preprocessed_casual_data.iloc[i, 0])\n",
    "\n",
    "preprocessed_casual_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sa7uZVo8nFJh"
   },
   "outputs": [],
   "source": [
    "conversations_class_Casual\n",
    "sample_Casual = pd.DataFrame(conversations_class_Casual)\n",
    "\n",
    "text = textLine_from_row(sample_Casual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2HdMYucnGbj"
   },
   "outputs": [],
   "source": [
    "py_hanspell_text = py_hanspell_spell_correct(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfcWZ8DlnMc6"
   },
   "outputs": [],
   "source": [
    "py_hanspell_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elfBdGT9nPsL"
   },
   "outputs": [],
   "source": [
    "df_py_casual = pd.DataFrame(py_hanspell_text)\n",
    "df_py_casual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cHF8FjpnTQo"
   },
   "source": [
    "#### [생략 가능] BaseOnPLMmodelET5_spell_correct\n",
    "- 실행 완료까지 오래 걸리므로 생략하시는 것을 추천 드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpVJvZownQNS"
   },
   "outputs": [],
   "source": [
    "ET5_text = BaseOnPLMmodelET5_spell_correct(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mTNOJULvnWMx"
   },
   "outputs": [],
   "source": [
    "df_ET5_casual = pd.DataFrame(ET5_text)\n",
    "df_ET5_casual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YWIn7P8LEI7"
   },
   "source": [
    "# 전처리(2) - 품사 태깅 및 필터링\n",
    "- 최종 POS Tagger 및  : bareun 형태소 분석 API\n",
    "- 품사 필터링 : AntConc에서 키워드 리스트를 생성하기 위해, 품사를 필터링한 후 각 단어 옆에 품사 태그를 붙이는 방식으로 데이터를 변환하였음\n",
    "\n",
    "- 남긴 품사 : 명사(NNG, NNP), 동사(VV), 형용사(VA) (조정 가능함)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oREkC7zcNRdO"
   },
   "source": [
    "## [환경 설정] 전처리(2) 품사 태깅 및 필터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bQvY44iVJuR"
   },
   "source": [
    "### API key 입력 후 정상 작동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FiF4e_G6TiE4"
   },
   "outputs": [],
   "source": [
    "# 아래에 \"https://bareun.ai/\"에서 이메일 인증 후 발급받은 API KEY(\"koba-...\")를 입력해주세요. \"로그인-내정보 확인\"\n",
    "\n",
    "API_KEY = \"koba-MDIHR4Q-BQUUMOY-TOGE42A-NDESPKY\" # <- 본인의 API KEY로 교체\n",
    "# API_KEY = \"koba-MDIHR4Q-BQUUMOY-TOGE42A-NDESPKY\"\n",
    "t = brn.Tagger(API_KEY, \"localhost\", 5656)\n",
    "\n",
    "# sample code : debugging\n",
    "res = t.tags([\"안녕하세요. 정말 좋은 날씨네요.\"])\n",
    "m = res.msg()\n",
    "tf.PrintMessage(m, out=sys.stdout, as_utf8=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxWE8mnFUhMX"
   },
   "source": [
    "### API에서 분석 받은 객체의 정보를 확인\n",
    "\n",
    "- length of tokens in sentences\n",
    "- length of morphemes of first token in sentences\n",
    "- lemma of first token in sentences\n",
    "- first morph of first token in sentences\n",
    "- tag of first morph of first token in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1STYZDUUXKI"
   },
   "outputs": [],
   "source": [
    "# 형태소 분석 실행한 객체를 json 형식으로 변환\n",
    "# sample_code : get json object\n",
    "jo = res.as_json()\n",
    "print(jo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E0Go2ySUw6D"
   },
   "outputs": [],
   "source": [
    "# 형태소 분석 실행한 객체를 튜플로 저장하거나, 명사, 동사만 추출\n",
    "# sample_code : get tuple of pos tagging\n",
    "pa = res.pos()\n",
    "print(pa)\n",
    "# another methods\n",
    "ma = res.morphs()\n",
    "print(ma)\n",
    "na = res.nouns()\n",
    "print(na)\n",
    "va = res.verbs()\n",
    "print(va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23remDthVRMv"
   },
   "source": [
    "## POS Tagger\n",
    "- tagger와 tokenizer를 명확히 분리한 이유: 품사 태깅까지만 되어 있는 데이터가 필요한 경우가 있음 (EDA 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLouEBn9WTGT"
   },
   "outputs": [],
   "source": [
    "# 태깅 코드\n",
    "tagger = brn.Tagger(API_KEY, \"localhost\", 5656)  # 위의 변수명('t')과 일치시킬 경우, 생략하여도 무방함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcLcEo8FWUhk"
   },
   "source": [
    "### sample code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pf-PhSTTj2K"
   },
   "outputs": [],
   "source": [
    "# print results\n",
    "res = tagger.tags([\"안녕하세요.\", \"반가워요!\"])\n",
    "print(res.msg()) # debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RikSKBdEV7PZ"
   },
   "outputs": [],
   "source": [
    "# get protobuf message.\n",
    "m = res.msg()\n",
    "tf.PrintMessage(m, out=sys.stdout, as_utf8=True)\n",
    "print(tf.MessageToString(m, as_utf8=True))\n",
    "print(f'length of sentences is {len(m.sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZmAUwGvWAwX"
   },
   "outputs": [],
   "source": [
    "## output : 2\n",
    "print(f'length of tokens in sentences[0] is {len(m.sentences[0].tokens)}')\n",
    "print(f'length of morphemes of first token in sentences[0] is {len(m.sentences[0].tokens[0].morphemes)}')\n",
    "print(f'lemma of first token in sentences[0] is {m.sentences[0].tokens[0].lemma}')\n",
    "print(f'first morph of first token in sentences[0] is {m.sentences[0].tokens[0].morphemes[0]}')\n",
    "print(f'tag of first morph of first token in sentences[0] is {m.sentences[0].tokens[0].morphemes[0].tag}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UF514e7mWZyR"
   },
   "source": [
    "### sample code : 사용자 지정 단어 사전 (생략)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwVY-zD_WQ2Q"
   },
   "outputs": [],
   "source": [
    "# custom dictionary\n",
    "cust_dic = tagger.custom_dict(\"my\")\n",
    "cust_dic.copy_np_set({'내고유명사', '우리집고유명사'})\n",
    "cust_dic.copy_cp_set({'코로나19'})\n",
    "cust_dic.copy_cp_caret_set({'코로나^백신', '독감^백신'})\n",
    "cust_dic.update()\n",
    "\n",
    "# laod prev custom dict\n",
    "cust_dict2 = tagger.custom_dict(\"my\")\n",
    "cust_dict2.load()\n",
    "\n",
    "tagger.set_domain('my')\n",
    "tagger.pos('코로나19는 언제 끝날까요?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDCNoZ98ZpDe"
   },
   "source": [
    "### 함수화 - json 객체 호출, 품사 getter, corpus 전체 태깅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJkWYN_ZZ2qg"
   },
   "outputs": [],
   "source": [
    "def tagging(tagger, sentences):\n",
    "    res = tagger.tags([sentences])  # 문장 전체 태깅\n",
    "    return res  # res 객체 반환\n",
    "\n",
    "\n",
    "def get_json(res): # 객체를 json 형식으로 변환\n",
    "    jo = res.as_json()\n",
    "    return jo\n",
    "\n",
    "\n",
    "def get_pos(res):  # 품사 태깅된 res instance로부터 형태소 추출\n",
    "    jo = get_json(res)\n",
    "    pa = res.pos()\n",
    "    ma = res.morphs()\n",
    "    na = res.nouns()\n",
    "    va = res.verbs()\n",
    "    return pa, ma, na, va, jo\n",
    "\n",
    "\n",
    "def corpus_tagging(corpus, tagger):  # corpus 전체, 데이터(row) 내부의 line 단위로 품사 태깅 객체 생성\n",
    "    tagged_data = []  # 태깅된 데이터를 저장할 리스트\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        sentences = corpus[i].split('\\n')  # '\\n' 기준으로 문장 분리\n",
    "        row_results = []  # 각 행에 대한 결과를 저장할 리스트\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if sentence.strip():  # 빈 문장 제외\n",
    "                res = tagging(tagger, sentence)\n",
    "                pa, ma, na, va, jo = get_pos(res)  # 품사 태깅 결과\n",
    "                row_results.append({\n",
    "                    'sentence': sentence,\n",
    "                    'pos': pa,\n",
    "                    # 'morphs': ma,  # 필요시 추가하여 사용\n",
    "                    # 'nouns': na,\n",
    "                    # 'verbs': va,\n",
    "                    # 'json': jo\n",
    "                })  # 태깅 결과를 딕셔너리로 저장\n",
    "\n",
    "        tagged_data.append(row_results)  # 각 행의 결과를 추가\n",
    "\n",
    "    return tagged_data  # 태깅된 데이터 반환\n",
    "\n",
    "\n",
    "def instance_info(res, i): # 필요시 사용\n",
    "    m = res.msg()  # res에서 msg() 호출\n",
    "    print(f'length of tokens in sentences[{i}] is {len(m.sentences)}')\n",
    "    if m.sentences:\n",
    "        print(f'length of morphemes of first token in sentences[{i}] is {len(m.sentences[0].tokens[0].morphemes)}')\n",
    "        print(f'lemma of first token in sentences[{i}] is {m.sentences[0].tokens[0].lemma}')\n",
    "        print(f'first morph of first token in sentences[{i}] is {m.sentences[0].tokens[0].morphemes[0].text.content}')\n",
    "        print(f'tag of first morph of first token in sentences[{i}] is {m.sentences[0].tokens[0].morphemes[0].tag}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-G33wNReZ7lL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqAxZN_5VbWr"
   },
   "source": [
    "## Filtering POS\n",
    "- 유의미한 품사만 남긴(태깅된) text 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUczZtZDLBqk"
   },
   "outputs": [],
   "source": [
    "def filter_pos(tagged_data, pos_to_keep):  # 품사 필터링 후 각 단어 옆에 품사 태그 붙여 반환함\n",
    "    filtered_data = []\n",
    "\n",
    "    for block in tagged_data:  # 여러 문장 블록을 포함하는 리스트\n",
    "        for entry in block:  # 각 문장별로 필터링\n",
    "            filtered_tokens = [\n",
    "                f\"{morph}/{pos}\" for morph, pos in entry['pos'] if pos in pos_to_keep\n",
    "            ]\n",
    "            filtered_data.append(' '.join(filtered_tokens))\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def save_to_csv(filtered_data, filename):  # 데이터 저장\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Filtered Morphs\"])  # 헤더 생성\n",
    "\n",
    "        for filtered_sentence in filtered_data:\n",
    "            writer.writerow([filtered_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Py9BOHLcNulU"
   },
   "outputs": [],
   "source": [
    "def apply_tagging_and_filter(conversations_class, tagger, pos_to_keep):\n",
    "\n",
    "    filtered_conversations = []  # 필터링된 데이터를 저장할 리스트\n",
    "\n",
    "    # 1. 품사 태깅 수행\n",
    "    tagged_data = corpus_tagging(conversations_class.tolist(), tagger)  # Series를 리스트로 변환\n",
    "\n",
    "    # 2. filter_pos 함수 적용\n",
    "    filtered_data = filter_pos(tagged_data, pos_to_keep)\n",
    "\n",
    "    # 3. 결과 저장\n",
    "    filtered_conversations.append(filtered_data)\n",
    "\n",
    "    return filtered_conversations\n",
    "\n",
    "# 필터링할 품사 선택 (예시: 명사(NNG, NNP), 동사(VV), 형용사(VA))\n",
    "pos_to_keep = ['NNG', 'NNP', 'VV', 'VA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6r8rQuSb-UW"
   },
   "outputs": [],
   "source": [
    "# 데이터가 'tagged_data'라는 변수에 있다고 가정\n",
    "# sample code (코퍼스 태깅)\n",
    "corpus00 = corpus[:3]\n",
    "tagged_results = corpus_tagging(corpus00, tagger)\n",
    "\n",
    "tagged_data = tagged_results\n",
    "\n",
    "# 필요한 품사만 필터링\n",
    "filtered_results = filter_pos(tagged_data, pos_to_keep)\n",
    "\n",
    "# 필터링 결과를 CSV로 저장 (Filtered Morphs만 저장)\n",
    "save_to_csv(filtered_results, 'filtered_pos_data_for_antconc.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zObL0Mwxbnco"
   },
   "source": [
    "#### [생략 가능] 클래스 별 분리 후 적용 원할 경우  -전체 데이터 행에 대하여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOXIKsq2c3lt"
   },
   "outputs": [],
   "source": [
    "# 클래스 분리하여 저장\n",
    "\n",
    "def train_class_split(train_data) :  # data를 class 별로 분리\n",
    "  train_classes = [pd.DataFrame() for _ in range(4)]\n",
    "\n",
    "  for i in range(4) :\n",
    "    train_classes[i] = train_data[train_data['class'] == i]\n",
    "\n",
    "  return train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72xwvGXgdCNm"
   },
   "outputs": [],
   "source": [
    "train_classes = train_class_split(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zM-3jtzodGnb"
   },
   "outputs": [],
   "source": [
    "train_classes[0].head()  # 잘 저장되었는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NZ2uFJpdbOf"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# 결과를 저장할 리스트\n",
    "conversations_all_classes = []\n",
    "\n",
    "# train_classes의 각 클래스에 대해 반복\n",
    "for i in range(len(train_classes)):\n",
    "    # 각 클래스에서 conversation 열만 추출\n",
    "    conversations = train_classes[i]['conversation']\n",
    "    conversations_all_classes.append(conversations)\n",
    "\n",
    "    print(conversations.head())\n",
    "\n",
    "# conversations_all_classes 리스트에는 각 클래스별로 conversation 열이 저장됨\n",
    "...\n",
    "\n",
    "# train_classes[0]에서 conversation 열만 추출\n",
    "conversations_class_0 = train_classes[0]['conversation']\n",
    "conversations_class_1 = train_classes[1]['conversation']\n",
    "conversations_class_2 = train_classes[2]['conversation']\n",
    "conversations_class_3 = train_classes[3]['conversation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgULE0LR5ppH"
   },
   "outputs": [],
   "source": [
    "conversations_class_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0APElU1daGx"
   },
   "outputs": [],
   "source": [
    "# 각 DataFrame을 CSV 파일로 저장\n",
    "for i, df in enumerate(train_classes):\n",
    "    df.to_csv(f'train_class_{i}.csv', index=False)\n",
    "\n",
    "# 각 CSV 파일에 대한 다운로드 링크 제공 (Jupyter 환경에서 실행 시)\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "for i in range(len(train_classes)):\n",
    "    display(FileLink(f'train_class_{i}.csv')) # 링크 에러시, 코랩 폴더 열면, 파일이 생성되어 있을 것임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w06XTMHVbqxK"
   },
   "outputs": [],
   "source": [
    "conv_tagged_0 = apply_tagging_and_filter(conversations_class_0, tagger, pos_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qyJmuRSbscG"
   },
   "outputs": [],
   "source": [
    "conv_tagged_1 = apply_tagging_and_filter(conversations_class_1, tagger, pos_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVK8uvrSbtT6"
   },
   "outputs": [],
   "source": [
    "conv_tagged_2 = apply_tagging_and_filter(conversations_class_2, tagger, pos_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiKXuJdCbtGj"
   },
   "outputs": [],
   "source": [
    "conv_tagged_3 = apply_tagging_and_filter(conversations_class_3, tagger, pos_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sc-lkG3Jbs6X"
   },
   "outputs": [],
   "source": [
    "# 필터링 결과를 CSV로 저장 (Filtered Morphs만 저장)\n",
    "save_to_csv(conv_tagged_0, 'filtered_pos_data_for_antconc_0.csv')\n",
    "save_to_csv(conv_tagged_1, 'filtered_pos_data_for_antconc_1.csv')\n",
    "save_to_csv(conv_tagged_2, 'filtered_pos_data_for_antconc_2.csv')\n",
    "save_to_csv(conv_tagged_3, 'filtered_pos_data_for_antconc_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1JODL5hPui6"
   },
   "source": [
    "# 전처리(3) - 형태소 분석, Tokenizer\n",
    "- 최종 형태소 분석기 : bareun API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivLUNh8TY-It"
   },
   "outputs": [],
   "source": [
    "# 토크나이징 코드\n",
    "\n",
    "# API_KEY = \"koba-MDIHR4Q-BQUUMOY-TOGE42A-NDESPKY\" # Yeonwoo's API KEY\n",
    "\n",
    "tokenizer = brn.Tokenizer(API_KEY, \"localhost\", 5656)   # tokenizer 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Oa-Wo_XZIbL"
   },
   "source": [
    "### 샘플 코드 : 정상 작동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T_QrSZ6ZDmR"
   },
   "outputs": [],
   "source": [
    "# print results.\n",
    "tokenized = tokenizer.tokenize_list([\"안녕하세요.\", \"반가워요!\"])\n",
    "\n",
    "# get protobuf message.\n",
    "m = tokenized.msg()\n",
    "tf.PrintMessage(m, out=sys.stdout, as_utf8=True)\n",
    "print(tf.MessageToString(m, as_utf8=True))\n",
    "print(f'length of sentences is {len(m.sentences)}')\n",
    "## output : 2\n",
    "print(f'length of tokens in sentences[0] is {len(m.sentences[0].tokens)}')\n",
    "print(f'length of segments of first token in sentences[0] is {len(m.sentences[0].tokens[0].segments)}')\n",
    "print(f'tagged of first token in sentences[0] is {m.sentences[0].tokens[0].tagged}')\n",
    "print(f'first segment of first token in sentences[0] is {m.sentences[0].tokens[0].segments[0]}')\n",
    "print(f'hint of first morph of first token in sentences[0] is {m.sentences[0].tokens[0].segments[0].hint}')\n",
    "\n",
    "## Advanced usage.\n",
    "for sent in m.sentences:\n",
    "    for token in sent.tokens:\n",
    "        for m in token.segments:\n",
    "            print(f'{m.text.content}/{m.hint}')\n",
    "\n",
    "# get json object\n",
    "jo = tokenized.as_json()\n",
    "print(jo)\n",
    "\n",
    "# get tuple of segments\n",
    "ss = tokenized.segments()\n",
    "print(ss)\n",
    "ns = tokenized.nouns()\n",
    "print(ns)\n",
    "vs = tokenized.verbs()\n",
    "print(vs)\n",
    "# postpositions: 조사\n",
    "ps = tokenized.postpositions()\n",
    "print(ps)\n",
    "# Adverbs, 부사\n",
    "ass = tokenized.adverbs()\n",
    "print(ass)\n",
    "ss = tokenized.symbols()\n",
    "print(ss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDgFBRDMZWn5"
   },
   "source": [
    "### tokenizer\n",
    "- DLThon 코퍼스 전체 적용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOCYHkTvZVwW"
   },
   "outputs": [],
   "source": [
    "def tagging_and_tokenizing(tagger, tokenizer, sentences):\n",
    "    # 품사 태깅\n",
    "    tagged_res = tagger.tags([sentences])\n",
    "    pa, ma, na, va, jo = get_pos(tagged_res)\n",
    "\n",
    "    # 토큰화\n",
    "    tokenized = tokenizer.tokenize_list([sentences])\n",
    "    tokenized_msg = tokenized.msg()\n",
    "\n",
    "    # tokenized 결과 추출\n",
    "    tokens_info = []\n",
    "    for token in tokenized_msg.sentences[0].tokens:\n",
    "        tokens_info.append({\n",
    "            'text': token.text.content,\n",
    "            'segments': [seg.text.content for seg in token.segments],\n",
    "            'tagged': token.tagged\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        'pos': pa,\n",
    "        'morphs': ma,\n",
    "        'nouns': na,\n",
    "        'verbs': va,\n",
    "        'json': jo,\n",
    "        'tokens_info': tokens_info\n",
    "    }\n",
    "\n",
    "def corpus_tagging_and_tokenizing(corpus, tagger, tokenizer):\n",
    "    tagged_data = []  # 태깅된 데이터를 저장할 리스트\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        sentences = corpus[i].split('\\n')  # '\\n' 기준으로 문장 분리\n",
    "        row_results = []  # 각 행에 대한 결과를 저장할 리스트\n",
    "        for sentence in sentences:\n",
    "            if sentence.strip():  # 빈 문장 제외\n",
    "                result = tagging_and_tokenizing(tagger, tokenizer, sentence)\n",
    "                row_results.append({\n",
    "                    'sentence': sentence,\n",
    "                    'pos': result['pos'],\n",
    "                    'morphs': result['morphs'],\n",
    "                    'nouns': result['nouns'],\n",
    "                    'verbs': result['verbs'],\n",
    "                    'json': result['json'],\n",
    "                    'tokens_info': result['tokens_info']\n",
    "                })\n",
    "\n",
    "        tagged_data.append(row_results)  # 각 행의 결과를 추가\n",
    "\n",
    "    return tagged_data  # 태깅된 데이터 반환\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUIvO--zgWRU"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 태거 및 토크나이저 객체 생성 (가정된 API 호출)\n",
    "# tagger = brn.Tagger(API_KEY, \"localhost\", 5656)\n",
    "# tokenizer = brn.Tokenizer(API_KEY, \"localhost\", 5656)\n",
    "\n",
    "# 예시 실행 (코퍼스 태깅과 토큰화)\n",
    "corpus_tt = corpus[:3] # 정상 동작 확인 후 아래 코드 진행\n",
    "# corpus_tt = corpus\n",
    "\n",
    "tagged_and_tokenized_results = corpus_tagging_and_tokenizing(corpus_tt, tagger, tokenizer)\n",
    "\n",
    "# 결과 확인\n",
    "print(tagged_and_tokenized_results)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2bQvY44iVJuR",
    "GxWE8mnFUhMX",
    "PcLcEo8FWUhk",
    "UF514e7mWZyR",
    "JDCNoZ98ZpDe",
    "zObL0Mwxbnco",
    "3Oa-Wo_XZIbL"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
